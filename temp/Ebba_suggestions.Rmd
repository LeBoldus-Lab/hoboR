## Ebba suggestions for data cleaning

Remove "impossible" values.  I usually screen all data for values I consider to be impossible or null. Having functions whereby you can define what you consider "impossible" would be helpful. 

After that, I typically rely on visual aids to ID errors, particularly for those errors which aren't impossible over the year but would be considered highly unlikely given the season.  You could consider adding some automated ways to produce visualoutput, or use a moving average to ID those values which fall outside the range of neighboring values in the timeseries. 

The two methods I rely on comprise mainly:  

Trends.  It's helpful to me to just graph the    
         entirety of the dataset as a time 
         series to ID any spikes in the data.   
         Obviously, there's a limit to how 
         much data can be looked at in one time, but          I've processed a year of data taken in ½  
         hour intervals in this manor with 0 issues.
         This is most useful with an ID function  
         (such as in Excel when I can hover over a 
         point and have it pop out the X,Y for that          point).  
         It might not need to be visual though: most          weather data I've seen has a fairly smooth          transition and any large changes from a  
         previous measurement would be suspect.   
         Perhaps you could tag suspicious values 
         by comparing the next value in a timeseries          to the average of a preceding set of 
         values.  Anything greater than 1 or 2 s.d. 
         from that average would be tagged as    
         suspect.  

Correlation.  If you are summarizing data over a  
         given time period (i.e. weekly  
         temperature), also calculating the max/min          over that time period will be useful ID 
         anomalies.  The erroneous temperature  
         reading popped out in 
         Adam's data because the single max-value  
         was so off the average for 
         that same time period and off-trend for the          data as a whole.  
         This is most applicable to situations when          you would expect to be highly correlated   
         (such as comparing max temp to average  
         temp).

Other features that would be helpful would include the following. Perhaps you've already thought of this, but these come to mind: 

    It would be helpful to be able to extract a    
    single time stamp into multiple 
    variables of different duration.  For example, I     was collecting data in ½ hour 
    intervals for one project, meaning for each day     I have 48 observations.  
    The program I was working with provided date &      time information as single variable 
    (e.g. February 22, 2020 20:30) for each   
    observation, even though 
    I wanted to summarize the information by day.  I     find it useful to have those pieces of    
    information extracted into separate variables   
    for each observation indicating day of year   
    (53), month (February), week of year (8), hour      of day (20:00), etc.
    It looks like you already set this up for Adam's     data, but it would be helpful to be able to         define the start/stop dates & times for        
    individual sample periods and assign sample IDs     to each observation. Ideally each station could     be coded with different start/stop periods to   
    account for irregular duration by station 
    (i.e. when you can't get to all stations in a   
    single day / time).  (Of course, if this is what     you tried to do with Adam's data something went     awry... so having a check in place appears to be     important too!)

Once you have a defined sampling period, then having functions which would add another variable indicating the number of days or hours an event happens over that period would be cool.  For example, Adam could have a variable indicating 
the "number of days with measurable precipitation" or "number of hours above X degrees" within each sample period.